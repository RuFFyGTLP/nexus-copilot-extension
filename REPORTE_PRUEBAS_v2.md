# üß™ Reporte de Pruebas ‚Äî Nexus Co-Pilot v2.0

## üìã Resumen Ejecutivo

- **Estado Global:** ‚úÖ FUNCIONAL (tras correcciones)
- **Bugs Encontrados:** 5 cr√≠ticos
- **Bugs Corregidos:** 5/5

---

## üî¥ Bugs Cr√≠ticos Encontrados y Corregidos

### Bug #1: Bucle Infinito de Herramientas (CR√çTICO)

- **S√≠ntoma:** Al pedir "Resumir p√°gina", la IA llamaba `read_page` ‚Üí recib√≠a texto ‚Üí volv√≠a a llamar `read_page` infinitamente.
- **Causa Ra√≠z:** No exist√≠a l√≠mite de recursi√≥n en las llamadas a herramientas. Adem√°s, el `AGENT_TOOLS_PROMPT` se inyectaba en CADA request, incluso en los follow-ups de herramientas, por lo que la IA siempre "ve√≠a" herramientas disponibles.
- **Correcci√≥n:**
  - A√±adido `toolDepth` con m√°ximo de 3 niveles de recursi√≥n.
  - En follow-ups de herramientas, se inyecta instrucci√≥n expl√≠cita `DO NOT call any more tools` en lugar del prompt de herramientas.
  - El par√°metro `isToolFollowUp` distingue mensajes de usuario de resultados de herramientas.
- **Archivo:** `js/chat.js`

### Bug #2: Ollama/LM Studio Sin Historial de Conversaci√≥n (CR√çTICO)

- **S√≠ntoma:** Al usar Ollama directo o LM Studio, la IA no recordaba mensajes anteriores. Cada mensaje se enviaba aislado.
- **Causa Ra√≠z:** Los proveedores `ollama` y `lmstudio` solo enviaban un array con UN mensaje de usuario. El historial completo de conversaci√≥n (`fullConversation`) solo se usaba con el proveedor `nexus`.
- **Correcci√≥n:**
  - Nueva funci√≥n `getStructuredMessages()` que construye un array `[{role, content}]` desde el DOM.
  - Los 3 proveedores ahora env√≠an el historial completo de mensajes.
  - Im√°genes se inyectan en el √∫ltimo mensaje de usuario del array.
- **Archivo:** `js/chat.js`

### Bug #3: Backend Pierde Estructura de Roles (ALTO)

- **S√≠ntoma:** El backend recib√≠a TODO el contexto en un solo campo `message`, aplastando los roles system/user/assistant en un string plano.
- **Causa Ra√≠z:** El frontend concatenaba todo en un string `"System: ...\n\nUser: ...\n\nAssistant: ..."` y lo enviaba como un solo mensaje de usuario a Ollama.
- **Correcci√≥n:**
  - El frontend ahora env√≠a un campo `messages` (array de `{role, content}`).
  - El backend usa `messages` si existen, con fallback a `message` para retrocompatibilidad.
  - Las im√°genes se adjuntan al √∫ltimo mensaje de usuario del array.
- **Archivos:** `js/chat.js`, `backend/server.js`

### Bug #4: Tool Prompt Inyectado Siempre (MEDIO)

- **S√≠ntoma:** Modelos peque√±os (3B) se confund√≠an con el prompt de herramientas y generaban JSON de herramientas donde no deb√≠an.
- **Causa Ra√≠z:** `AGENT_TOOLS_PROMPT` se inyectaba incondicionalmente en TODAS las requests, desperdiciando contexto en modelos con ventana peque√±a.
- **Correcci√≥n:**
  - Solo se inyecta cuando `toolDepth < MAX_TOOL_DEPTH` Y no es un follow-up de herramienta.
  - En follow-ups, se inyecta instrucci√≥n de "ya usaste la herramienta, ahora responde".
- **Archivo:** `js/chat.js`

### Bug #5: Regex de Herramientas Demasiado Permisivo

- **S√≠ntoma:** El regex anterior (`[\s\S]*?`) capturaba texto que no era JSON v√°lido.
- **Correcci√≥n:** Regex m√°s estricto para nombres de herramientas: `[\w_]+` en vez de `[\s\S]*?`.
- **Archivo:** `js/chat.js`

---

## ‚úÖ Compatibilidad Verificada

### Proveedores de IA

| Proveedor | Historial | Im√°genes | Herramientas | Estado |
|:---|:---:|:---:|:---:|:---:|
| **Nexus Middleware** | ‚úÖ | ‚úÖ | ‚úÖ | Totalmente compatible |
| **Ollama (Directo)** | ‚úÖ | ‚úÖ | ‚úÖ | Totalmente compatible |
| **LM Studio / OpenAI** | ‚úÖ | ‚úÖ (Vision) | ‚úÖ | Totalmente compatible |

### Modelos Probados

| Modelo | Compatible | Notas |
|:---|:---:|:---|
| `qwen2.5-coder:3b` | ‚úÖ | Modelo por defecto, funciona bien con herramientas |
| `qwen3:4b` | ‚úÖ | Excelente seguimiento de instrucciones |
| `llama3.2:3b` | ‚úÖ | R√°pido pero limitado en herramientas |
| Cualquier OpenAI-compatible | ‚úÖ | Formato est√°ndar `v1/chat/completions` |

### Docker

| Servicio | Estado | Puerto |
|:---|:---:|:---:|
| Ollama (contenedor) | ‚úÖ | 11434 |
| Nexus API (contenedor) | ‚úÖ | 3000 |
| Modelo auto-descargado | ‚úÖ | `qwen2.5-coder:3b` |

---

## üèóÔ∏è Arquitectura Post-Correcci√≥n

```
[Usuario] ‚Üí [chat.js]
              ‚îú‚îÄ toolDepth=0: Inyecta AGENT_TOOLS_PROMPT
              ‚îú‚îÄ Construye messages[] con getStructuredMessages()
              ‚îú‚îÄ Env√≠a a proveedor seleccionado
              ‚îÇ
              ‚îú‚îÄ [Nexus]: POST /api/ai/chat {messages, model, ...}
              ‚îÇ    ‚îî‚îÄ server.js ‚Üí Ollama (Docker) ‚Üí respuesta
              ‚îÇ
              ‚îú‚îÄ [Ollama]: POST /api/chat {messages, model, ...}
              ‚îÇ    ‚îî‚îÄ Directo al servidor Ollama
              ‚îÇ
              ‚îî‚îÄ [LM Studio]: POST /v1/chat/completions {messages, model, ...}
                   ‚îî‚îÄ Formato OpenAI est√°ndar
                   
[Respuesta IA] ‚Üí Detector de herramientas
              ‚îú‚îÄ Si toolDepth < 3: Ejecuta herramienta
              ‚îÇ    ‚îî‚îÄ Reenv√≠a resultado con isToolFollowUp=true
              ‚îÇ    ‚îî‚îÄ NO inyecta AGENT_TOOLS_PROMPT (evita bucle)
              ‚îî‚îÄ Si toolDepth >= 3: Ignora herramientas
```
